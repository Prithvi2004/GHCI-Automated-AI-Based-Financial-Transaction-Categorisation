"""
Comprehensive Model Evaluation Pipeline
Generates detailed metrics, confusion matrix, and analysis reports
"""

import pandas as pd
import joblib
import json
import os
from src.preprocess import preprocess_dataframe
from src.evaluate import generate_evaluation_report
from src.explainability import TransactionExplainer, demonstrate_explainability
from src.bias_detection import BiasAnalyzer, demonstrate_bias_analysis
from src.performance import PerformanceBenchmark, demonstrate_benchmarking
from src.robustness import RobustnessTest, demonstrate_robustness_testing


def main():
    print("="*70)
    print("üìä TRANSACTION CATEGORIZATION - COMPREHENSIVE EVALUATION")
    print("="*70)
    print()
    
    # Load test data and model
    print("üìÇ Loading test data and model...")
    
    if not os.path.exists("data/test_transactions.csv"):
        print("‚ùå Test data not found. Please run train_model.py first.")
        return
    
    if not os.path.exists("model.pkl"):
        print("‚ùå Model not found. Please run train_model.py first.")
        return
    
    test_df = pd.read_csv("data/test_transactions.csv")
    test_df = preprocess_dataframe(test_df, text_col='description')
    categorizer = joblib.load("model.pkl")
    
    print(f"   ‚úÖ Loaded {len(test_df)} test samples")
    print(f"   ‚úÖ Model loaded successfully")
    print()
    
    # Generate predictions
    print("üîÆ Generating predictions...")
    predictions, confidences = categorizer.predict(test_df['description'])
    test_df['predicted'] = predictions
    test_df['confidence'] = confidences
    print("   ‚úÖ Predictions complete")
    print()
    
    # ========== Core Evaluation ==========
    print("="*70)
    print("1Ô∏è‚É£  CORE PERFORMANCE METRICS")
    print("="*70)
    
    report = generate_evaluation_report(
        test_df['category'], 
        predictions, 
        categorizer.get_categories()
    )
    
    print(f"\nüìà MACRO F1-SCORE: {report['macro_f1']:.4f}")
    print(f"   Target: ‚â• 0.90 | Status: {'‚úÖ ACHIEVED' if report['macro_f1'] >= 0.90 else '‚ö†Ô∏è BELOW TARGET'}")
    print()
    
    print("üìä Per-Category F1 Scores:")
    for cat, f1 in sorted(report['per_class_f1'].items(), key=lambda x: x[1], reverse=True):
        bar = "‚ñà" * int(f1 * 20)
        print(f"   {cat:20s} {f1:.3f} {bar}")
    print()
    
    # Save evaluation report
    with open("evaluation_report.json", "w") as f:
        json.dump(report, f, indent=2)
    print("üíæ Detailed metrics saved to: evaluation_report.json")
    print()
    
    # ========== Explainability Analysis ==========
    print("="*70)
    print("2Ô∏è‚É£  EXPLAINABILITY ANALYSIS")
    print("="*70)
    
    sample_transactions = test_df['description'].sample(min(5, len(test_df))).tolist()
    demonstrate_explainability(categorizer, sample_transactions)
    print()
    
    # ========== Bias Detection ==========
    print("="*70)
    print("3Ô∏è‚É£  BIAS DETECTION & FAIRNESS ANALYSIS")
    print("="*70)
    
    bias_report = demonstrate_bias_analysis(categorizer, test_df)
    
    with open("bias_report.txt", "w") as f:
        f.write(bias_report)
    print("üíæ Bias analysis saved to: bias_report.txt")
    print()
    
    # ========== Performance Benchmarking ==========
    print("="*70)
    print("4Ô∏è‚É£  PERFORMANCE BENCHMARKING")
    print("="*70)
    
    perf_report = demonstrate_benchmarking(categorizer, test_df['description'].tolist())
    
    with open("performance_report.txt", "w") as f:
        f.write(perf_report)
    print("üíæ Performance benchmark saved to: performance_report.txt")
    print()
    
    # ========== Robustness Testing ==========
    print("="*70)
    print("5Ô∏è‚É£  ROBUSTNESS TESTING")
    print("="*70)
    
    robustness_report = demonstrate_robustness_testing(categorizer, test_df)
    
    with open("robustness_report.txt", "w") as f:
        f.write(robustness_report)
    print("üíæ Robustness analysis saved to: robustness_report.txt")
    print()
    
    # ========== Summary ==========
    print("="*70)
    print("üìã EVALUATION SUMMARY")
    print("="*70)
    print()
    print("‚úÖ Core Metrics:")
    print(f"   ‚Ä¢ Macro F1-Score: {report['macro_f1']:.4f}")
    print(f"   ‚Ä¢ Accuracy: {report['classification_report']['accuracy']:.4f}")
    print(f"   ‚Ä¢ Weighted F1: {report['classification_report']['weighted avg']['f1-score']:.4f}")
    print()
    print("‚úÖ Generated Reports:")
    print("   ‚Ä¢ evaluation_report.json - Detailed metrics and confusion matrix")
    print("   ‚Ä¢ bias_report.txt - Fairness analysis")
    print("   ‚Ä¢ performance_report.txt - Throughput and latency benchmarks")
    print("   ‚Ä¢ robustness_report.txt - Noise and variation testing")
    print("   ‚Ä¢ benchmark_results.json - Detailed performance data")
    print()
    print("‚úÖ Key Findings:")
    
    if report['macro_f1'] >= 0.90:
        print("   üéØ Macro F1-score exceeds 0.90 target")
    else:
        print(f"   ‚ö†Ô∏è Macro F1-score ({report['macro_f1']:.3f}) below 0.90 target")
        print("      Consider: more training data, feature engineering, or hyperparameter tuning")
    
    low_conf_pct = (confidences < 0.85).sum() / len(confidences)
    print(f"   üìä Low confidence predictions: {low_conf_pct:.1%}")
    
    if low_conf_pct > 0.2:
        print("      ‚ö†Ô∏è Consider human-in-the-loop feedback for low-confidence predictions")
    
    print()
    print("="*70)
    print("‚úÖ EVALUATION COMPLETE!")
    print("="*70)
    print()


if __name__ == "__main__":
    main()
